## 📝 Technical Articles on Medium

As part of my work in building scalable data platforms across healthcare and finance, I regularly write about real-time streaming, PySpark, Delta Lake, and modern data engineering practices.

These articles are meant to share real-world use cases, best practices, and hands-on code patterns for engineers working with big data at scale.

### 📚 Published Posts

- 🔄 **[Designing Scalable PySpark Pipelines on Azure Databricks for Healthcare Analytics]**(https://medium.com/@santhoshkumarv/designing-scalable-pyspark-pipelines-on-azure-databricks-for-healthcare-analytics-e7e977a558eb)
  - Learn how the real-time pipeline works in Healthcare and Financial sectors.

- 🔄 **[Handling Bad Records in Streaming Pipelines Using Dead Letter Queues in PySpark]**((https://medium.com/@santhoshkumarv/handling-bad-records-in-streaming-pipelines-using-dead-letter-queues-in-pyspark-265e7a55eb29 )
  - Learn how to isolate and manage malformed records in real-time pipelines using PySpark and Delta Lake.

- ⚡️ Coming Soon: Real-Time Pipeline Optimization in Databricks
  - Covers adaptive batching, watermarking, Z-ordering, and DLQ reprocessing.

- 🧪 Upcoming: Monitoring and Observability for Data Streams
  - How to set up alerts and dashboards for DLQ volumes and latency using tools like Grafana and Datadog.

---

📌 *Follow me on [Medium]( https://medium.com/@santhoshkumarv ) for more posts on PySpark, Delta Lake, Kafka, and real-time data architecture.*
